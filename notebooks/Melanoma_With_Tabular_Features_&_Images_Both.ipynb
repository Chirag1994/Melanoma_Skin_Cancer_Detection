{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing External Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2023-04-09T07:20:42.343744Z",
     "iopub.status.busy": "2023-04-09T07:20:42.343091Z",
     "iopub.status.idle": "2023-04-09T07:20:58.270670Z",
     "shell.execute_reply": "2023-04-09T07:20:58.269516Z",
     "shell.execute_reply.started": "2023-04-09T07:20:42.343704Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install efficientnet_pytorch --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-09T07:20:59.784469Z",
     "iopub.status.busy": "2023-04-09T07:20:59.784073Z",
     "iopub.status.idle": "2023-04-09T07:21:07.649298Z",
     "shell.execute_reply": "2023-04-09T07:21:07.647967Z",
     "shell.execute_reply.started": "2023-04-09T07:20:59.784430Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.0 0.14.0\n"
     ]
    }
   ],
   "source": [
    "################ Importing Libraries ################\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import random\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import albumentations as A\n",
    "import torch.cuda.amp as amp\n",
    "from torchinfo import summary\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageFile\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from timeit import default_timer as timer\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, MinMaxScaler\n",
    "!mkdir Models\n",
    "\n",
    "pd.set_option('expand_frame_repr', False)\n",
    "print(torch.__version__, torchvision.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-09T07:21:07.652976Z",
     "iopub.status.busy": "2023-04-09T07:21:07.651839Z",
     "iopub.status.idle": "2023-04-09T07:21:07.767762Z",
     "shell.execute_reply": "2023-04-09T07:21:07.766455Z",
     "shell.execute_reply.started": "2023-04-09T07:21:07.652933Z"
    }
   },
   "outputs": [],
   "source": [
    "## Configuration Settings ##\n",
    "\n",
    "class Config:\n",
    "        \n",
    "    EPOCHS = 20\n",
    "    IMG_SIZE = 512\n",
    "    ES_PATIENCE = 2\n",
    "    WEIGHT_DECAY = 0.001\n",
    "    VAL_BATCH_SIZE = 32 * 2\n",
    "    RANDOM_STATE = 1994\n",
    "    LEARNING_RATE = 5e-5\n",
    "    TRAIN_BATCH_SIZE = 32\n",
    "    MEAN = (0.485, 0.456, 0.406)\n",
    "    STD = (0.229, 0.224, 0.225)\n",
    "    TRAIN_COLS = [\"image_name\", \"patient_id\", \"sex\", \"age_approx\", \"anatom_site_general_challenge\",\n",
    "                 \"target\", \"tfrecord\"]\n",
    "    TEST_COLS = [\"image_name\", \"patient_id\", \"sex\", \"age_approx\", \"anatom_site_general_challenge\"]\n",
    "    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    ################ Setting paths to data input ################\n",
    "    \n",
    "    data_2020 = \"/kaggle/input/jpeg-melanoma-512x512/\"\n",
    "    train_folder_2020 = data_2020 + \"train/\"\n",
    "    test_folder_2020 = data_2020 + \"test/\"\n",
    "    test_csv_path_2020 = data_2020 + \"test.csv\"\n",
    "    train_csv_path_2020 = data_2020 + \"train.csv\"\n",
    "    submission_csv_path = data_2020 + \"sample_submission.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-09T07:21:07.770364Z",
     "iopub.status.busy": "2023-04-09T07:21:07.769656Z",
     "iopub.status.idle": "2023-04-09T07:21:07.794622Z",
     "shell.execute_reply": "2023-04-09T07:21:07.793506Z",
     "shell.execute_reply.started": "2023-04-09T07:21:07.770324Z"
    }
   },
   "outputs": [],
   "source": [
    "## Helper Utilities\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "class EarlyStopping:\n",
    "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\n",
    "       Directly borrowed from https://github.com/Bjarten/early-stopping-pytorch/blob/master/pytorchtools.py\"\"\"\n",
    "    def __init__(self, path, patience=7, verbose=False, delta=0, trace_func=print):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "                            Default: 7\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
    "                            Default: False\n",
    "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "                            Default: 0\n",
    "            path (str): Path for the checkpoint to be saved to.\n",
    "                            Default: 'checkpoint.pt'\n",
    "            trace_func (function): trace print function.\n",
    "                            Default: print            \n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "        self.trace_func = trace_func\n",
    "    def __call__(self, val_loss, model):\n",
    "\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "            \n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        '''Saves model when validation loss decrease.'''\n",
    "        if self.verbose:\n",
    "            self.trace_func(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        torch.save(obj=model.state_dict(), f=self.path)\n",
    "        self.val_loss_min = val_loss\n",
    "        \n",
    "def plot_loss_curves(results):\n",
    "    \"\"\"\n",
    "    Function to plot training & validation loss curves & validation AUC\n",
    "    \"\"\"\n",
    "    loss = results['train_loss']\n",
    "    valid_loss = results['valid_loss']\n",
    "    # Get the accuracy values of the results dictionary (training and test)\n",
    "    valid_auc = results['valid_auc']\n",
    "    # Figure out how many epochs there were\n",
    "    epochs = range(len(results['train_loss']))\n",
    "    # Setup a plot \n",
    "    plt.figure(figsize=(15, 7))\n",
    "    # Plot loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, loss, label='train_loss')\n",
    "    plt.plot(epochs, valid_loss, label='valid_loss')\n",
    "    plt.title('Loss'); plt.xlabel('Epochs');plt.legend()\n",
    "    # Plot accuracy\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, valid_auc, label='valid_auc')\n",
    "    plt.title('AUC Score'); plt.xlabel('Epochs'); plt.legend();\n",
    "    \n",
    "class RareLabelCategoryEncoder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, variables, tol=0.05):\n",
    "        if not isinstance(variables, list):\n",
    "            raise ValueError('Variables should be a list')\n",
    "        self.tol = tol\n",
    "        self.variables = variables\n",
    "    def fit(self, X, y=None):\n",
    "        self.encoder_dict_ = {}\n",
    "        for var in self.variables:\n",
    "            t = pd.Series(X[var]).value_counts(normalize=True)\n",
    "            self.encoder_dict_[var] = list(t[t >= self.tol].index)\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        for var in self.variables:\n",
    "            X[var] = np.where(\n",
    "                X[var].isin(self.encoder_dict_[var]),\n",
    "                                        X[var], \"Other\")\n",
    "        return X\n",
    "    \n",
    "class OutlierTreatment(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, variable, upper_quantile=None, lower_quantile=None):\n",
    "        if not isinstance(variable, str):\n",
    "            raise ValueError(\"Variable should be a string type.\")\n",
    "        self.upper_quantile = upper_quantile\n",
    "        self.variable = variable\n",
    "        self.lower_quantile = lower_quantile\n",
    "    def fit(self, X, y=None):\n",
    "        self.upper_quantile = X[self.variable].quantile(self.upper_quantile)\n",
    "        self.lower_quantile = X[self.variable].quantile(self.lower_quantile)\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        X[self.variable] = np.where(\n",
    "                    X[self.variable] > self.upper_quantile, self.upper_quantile,\n",
    "                            np.where(X[self.variable] < self.lower_quantile, self.lower_quantile, X[self.variable]))\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-09T07:21:07.798414Z",
     "iopub.status.busy": "2023-04-09T07:21:07.797776Z",
     "iopub.status.idle": "2023-04-09T07:21:07.815105Z",
     "shell.execute_reply": "2023-04-09T07:21:07.814168Z",
     "shell.execute_reply.started": "2023-04-09T07:21:07.798377Z"
    }
   },
   "outputs": [],
   "source": [
    "## Creating Dataset classes to load the images\n",
    "\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "class DatasetRetriever(nn.Module):\n",
    "    def __init__(self, df, tabular_features=None, use_tabular_features=False,\n",
    "                augmentations=None, is_test=False):\n",
    "        self.df = df\n",
    "        self.tabular_features = tabular_features\n",
    "        self.use_tabular_features = use_tabular_features\n",
    "        self.augmentations = augmentations\n",
    "        self.is_test = is_test\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        image_path = self.df['image_path'].iloc[index]\n",
    "        image = Image.open(image_path)\n",
    "        image = np.array(image)\n",
    "        if self.augmentations is not None:\n",
    "            augmented = self.augmentations(image=image)\n",
    "            image = augmented['image']\n",
    "        image = np.transpose(image, (2,0,1)).astype(np.float32)\n",
    "        image = torch.tensor(image, dtype=torch.float)\n",
    "        if self.use_tabular_features:\n",
    "            if len(self.tabular_features) > 0 and self.is_test == False:\n",
    "                tabular_features = np.array(self.df.iloc[index][self.tabular_features].values, \n",
    "                                            dtype=np.float32)\n",
    "                targets = self.df.target[index]\n",
    "                return {\"image\": image, \"tabular_features\": tabular_features, \"targets\": torch.tensor(targets, \n",
    "                                                                                                      dtype=torch.long)}\n",
    "            elif len(self.tabular_features) > 0 and self.is_test == True:\n",
    "                tabular_features = np.array(self.df.iloc[index][self.tabular_features].values,\n",
    "                                            dtype=np.float32)\n",
    "                return {\"image\": image, \"tabular_features\": tabular_features}\n",
    "        else:\n",
    "            if self.is_test == False:\n",
    "                targets = self.df.target[index]\n",
    "                return {\"image\" : image, \"targets\": torch.tensor(targets, dtype=torch.long)}\n",
    "            elif self.is_test == True:\n",
    "                return {\"image\": image}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training & Evaluation Loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-09T07:21:07.817382Z",
     "iopub.status.busy": "2023-04-09T07:21:07.816693Z",
     "iopub.status.idle": "2023-04-09T07:21:07.839201Z",
     "shell.execute_reply": "2023-04-09T07:21:07.838155Z",
     "shell.execute_reply.started": "2023-04-09T07:21:07.817346Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_one_epoch(model, dataloader, loss_fn, optimizer, device, scaler, use_tabular_features=True):\n",
    "    train_loss = 0\n",
    "    model.train()\n",
    "    for batch, data in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        if use_tabular_features:\n",
    "            if batch == 0:\n",
    "                print(\"Using tabular features\")\n",
    "            data[\"image\"], data[\"tabular_features\"], data['targets'] = data[\"image\"].to(device, dtype=torch.float), \\\n",
    "                    data[\"tabular_features\"].to(device, dtype=torch.float), data['targets'].to(device, dtype=torch.float)\n",
    "            with amp.autocast():\n",
    "                y_logits = model(data['image'], data['tabular_features']).squeeze(dim=0)\n",
    "                loss = loss_fn(y_logits, data[\"targets\"].view(-1,1))\n",
    "        else:\n",
    "            if batch == 0:\n",
    "                print(\"Not using tabular features\")\n",
    "            data[\"image\"], data['targets'] = data[\"image\"].to(device, dtype=torch.float), data['targets'].to(device, \n",
    "                                                                                        dtype=torch.float)\n",
    "            with amp.autocast():\n",
    "                y_logits = model(data['image']).squeeze(dim=0)\n",
    "                loss = loss_fn(y_logits, data[\"targets\"].view(-1,1))\n",
    "        train_loss += loss.item()\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()        \n",
    "    train_loss = train_loss / len(dataloader)\n",
    "    return train_loss\n",
    "\n",
    "def validate_one_epoch(model, dataloader, loss_fn, device, use_tabular_features=True):\n",
    "    valid_loss, final_predictions = 0, []\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        for batch, data in enumerate(dataloader):\n",
    "            if use_tabular_features:\n",
    "                if batch == 0:\n",
    "                    print(\"Using tabular features\")\n",
    "                data[\"image\"], data[\"tabular_features\"], data['targets'] = data[\"image\"].to(device, dtype=torch.float), \\\n",
    "                    data[\"tabular_features\"].to(device, dtype=torch.float), data['targets'].to(device, dtype=torch.float)\n",
    "                y_logits = model(data['image'], data['tabular_features']).squeeze(dim=0)\n",
    "            else:\n",
    "                if batch == 0:\n",
    "                    print(\"Not using tabular features\")\n",
    "                data[\"image\"], data['targets'] = data[\"image\"].to(device, dtype=torch.float), data['targets'].to(device,\n",
    "                                                                                                    dtype=torch.float)    \n",
    "                y_logits = model(data['image']).squeeze(dim=0)\n",
    "            loss = loss_fn(y_logits, data[\"targets\"].view(-1,1))\n",
    "            valid_loss += loss.item()\n",
    "            valid_probs = torch.sigmoid(y_logits).detach().cpu().numpy()\n",
    "            final_predictions.extend(valid_probs)\n",
    "    valid_loss = valid_loss / len(dataloader)\n",
    "    return valid_loss, final_predictions\n",
    "\n",
    "def train(model, train_dataloader, valid_dataloader, loss_fn, optimizer, scheduler,\n",
    "          device, scaler, epochs, es_patience, model_save_path, validation_targets):\n",
    "    results = {\"train_loss\": [], \"valid_loss\": [], \"valid_auc\": []}\n",
    "    \n",
    "    early_stopping = EarlyStopping(patience=es_patience, verbose=True, path=model_save_path)\n",
    "    \n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        train_loss = train_one_epoch(model=model, dataloader=train_dataloader,\n",
    "                                    loss_fn=loss_fn, optimizer=optimizer,\n",
    "                                    device=device, scaler=scaler, use_tabular_features=True)\n",
    "        \n",
    "        valid_loss, valid_predictions = validate_one_epoch(model=model, \n",
    "                                    dataloader=valid_dataloader, loss_fn=loss_fn, device=device,\n",
    "                                                          use_tabular_features=True)\n",
    "        \n",
    "        valid_predictions = np.vstack(valid_predictions).ravel()\n",
    "        \n",
    "        valid_auc = roc_auc_score(y_score=valid_predictions, \n",
    "                                  y_true=validation_targets)\n",
    "        scheduler.step(valid_auc)\n",
    "        \n",
    "        early_stopping(valid_loss, model)\n",
    "        \n",
    "        if early_stopping.early_stop:\n",
    "            print(f\"Early Stopping\")\n",
    "            break\n",
    "            \n",
    "        model.load_state_dict(torch.load(model_save_path))\n",
    "        print(f\"Epoch : {epoch+1} | \"\n",
    "              f\"train_loss : {train_loss:.4f} | \"\n",
    "              f\"valid_loss : {valid_loss:.4f} | \"\n",
    "              f\"valid_auc : {valid_auc:.4f} \")\n",
    "        results['train_loss'].append(train_loss)\n",
    "        results['valid_loss'].append(valid_loss)\n",
    "        results['valid_auc'].append(valid_auc)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-09T07:21:07.841137Z",
     "iopub.status.busy": "2023-04-09T07:21:07.840798Z",
     "iopub.status.idle": "2023-04-09T07:21:07.856036Z",
     "shell.execute_reply": "2023-04-09T07:21:07.855023Z",
     "shell.execute_reply.started": "2023-04-09T07:21:07.841097Z"
    }
   },
   "outputs": [],
   "source": [
    "## Setting Training & Validation Augmentations\n",
    "\n",
    "training_augmentations = A.Compose([A.CoarseDropout(p=0.6),\n",
    "                                    A.RandomRotate90(p=0.6),\n",
    "                                    A.Flip(p=0.4),\n",
    "                                    A.OneOf([A.RandomBrightnessContrast(brightness_limit=0.2,\n",
    "                                                                       contrast_limit=0.3),\n",
    "                                            A.HueSaturationValue(hue_shift_limit=20,\n",
    "                                                                sat_shift_limit=60,\n",
    "                                                                val_shift_limit=50)], p=0.7),\n",
    "                                    A.OneOf([A.GaussianBlur(),\n",
    "                                            A.GaussNoise()], p=0.65),\n",
    "                                    A.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.35, rotate_limit=45, p=0.5),\n",
    "                                    A.OneOf([A.OpticalDistortion(p=0.3),\n",
    "                                            A.GridDistortion(p=0.1),\n",
    "                                            A.PiecewiseAffine(p=0.3)], p=0.7),\n",
    "    A.Normalize(mean=Config.MEAN, std=Config.STD,\n",
    "                max_pixel_value=255.0, always_apply=True)\n",
    "])\n",
    "\n",
    "validation_augmentations = A.Compose([A.Normalize(mean=Config.MEAN, std=Config.STD, \n",
    "                                        max_pixel_value=255.0,always_apply=True)\n",
    "                                     ])\n",
    "testing_augmentations = A.Compose([A.Normalize(mean=Config.MEAN, std=Config.STD, \n",
    "                                        max_pixel_value=255.0,always_apply=True)\n",
    "                                     ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-09T07:21:07.857892Z",
     "iopub.status.busy": "2023-04-09T07:21:07.857482Z",
     "iopub.status.idle": "2023-04-09T07:21:07.871932Z",
     "shell.execute_reply": "2023-04-09T07:21:07.870951Z",
     "shell.execute_reply.started": "2023-04-09T07:21:07.857857Z"
    }
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, model_name='efficientnet-b5', pool_type=F.adaptive_avg_pool2d,\n",
    "                num_tabular_features=0):\n",
    "        super().__init__()\n",
    "        self.pool_type = pool_type\n",
    "        self.model_name = model_name\n",
    "        self.backbone = EfficientNet.from_pretrained(model_name)\n",
    "        in_features = getattr(self.backbone, \"_fc\").in_features\n",
    "        if num_tabular_features>0:\n",
    "            self.meta = nn.Sequential(\n",
    "                nn.Linear(num_tabular_features, 512),\n",
    "                nn.BatchNorm1d(512),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(p=0.5),\n",
    "                nn.Linear(512, 128),\n",
    "                nn.BatchNorm1d(128),\n",
    "                nn.ReLU())\n",
    "            in_features += 128\n",
    "        self.output = nn.Linear(in_features, 1)\n",
    "    \n",
    "    def forward(self, image, tabular_features=None):\n",
    "        features = self.pool_type(self.backbone.extract_features(image), 1)\n",
    "        cnn_features = features.view(image.size(0),-1)\n",
    "        if num_tabular_features>0:\n",
    "            tabular_features = self.meta(tabular_features)\n",
    "            all_features = torch.cat((cnn_features, tabular_features), dim=1)\n",
    "            output = self.output(all_features)\n",
    "            return output\n",
    "        else:\n",
    "            output = self.output(cnn_features)\n",
    "            return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2023-04-09T07:21:07.874521Z",
     "iopub.status.busy": "2023-04-09T07:21:07.873736Z",
     "iopub.status.idle": "2023-04-09T07:21:08.059549Z",
     "shell.execute_reply": "2023-04-09T07:21:08.058431Z",
     "shell.execute_reply.started": "2023-04-09T07:21:07.874474Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Data points & Columns in Training dataset are - (33126, 7)\n",
      "\n",
      "Number of Data points & Columns in 2020 Testing dataset are - (10982, 5)\n",
      "\n",
      "Train Dataset Columns: ['image_name', 'patient_id', 'sex', 'age_approx', 'anatom_site_general_challenge', 'target', 'tfrecord']\n",
      "\n",
      "Test Dataset Columns: ['image_name', 'patient_id', 'sex', 'age_approx', 'anatom_site_general_challenge']\n",
      "\n",
      "Distribution of Target feature in training dataset are: \n",
      "0    98.237034\n",
      "1     1.762966\n",
      "Name: target, dtype: float64\n",
      "\n",
      "**********************************************************************\n",
      "\n",
      "Distribution of sex feature in training dataset are:\n",
      "male      51.560708\n",
      "female    48.243072\n",
      "NaN        0.196220\n",
      "Name: sex, dtype: float64\n",
      "\n",
      "**************************************************\n",
      "\n",
      "Distribution of sex feature in testing dataset are:\n",
      "male      56.956838\n",
      "female    43.043162\n",
      "Name: sex, dtype: float64\n",
      "\n",
      "Distribution of anatom_site_general_challenge feature in training dataset are:\n",
      "\n",
      "torso              50.851295\n",
      "lower extremity    25.409044\n",
      "upper extremity    15.042565\n",
      "head/neck           5.599831\n",
      "NaN                 1.590895\n",
      "palms/soles         1.132041\n",
      "oral/genital        0.374328\n",
      "Name: anatom_site_general_challenge, dtype: float64\n",
      "\n",
      "**********************************************************************\n",
      "\n",
      "Distribution of anatom_site_general_challenge feature in testing dataset are:\n",
      "\n",
      "torso              53.241668\n",
      "lower extremity    22.773630\n",
      "upper extremity    14.323438\n",
      "head/neck           5.244946\n",
      "NaN                 3.196139\n",
      "palms/soles         0.983427\n",
      "oral/genital        0.236751\n",
      "Name: anatom_site_general_challenge, dtype: float64\n",
      "**********************************************************************\n",
      "\n",
      "------------------- Missing values distribution in training dataset-------------------:\n",
      "image_name                         0\n",
      "patient_id                         0\n",
      "sex                               65\n",
      "age_approx                        68\n",
      "anatom_site_general_challenge    527\n",
      "target                             0\n",
      "tfrecord                           0\n",
      "dtype: int64\n",
      "\n",
      "-------------------Missing values distribution in testing dataset-------------------:\n",
      "image_name                         0\n",
      "patient_id                         0\n",
      "sex                                0\n",
      "age_approx                         0\n",
      "anatom_site_general_challenge    351\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "\n",
      "Viewing Training Dataset below\n",
      "\n",
      "     image_name  patient_id     sex  age_approx anatom_site_general_challenge  target  tfrecord\n",
      "0  ISIC_2637011  IP_7279968    male        45.0                     head/neck       0         0\n",
      "1  ISIC_0015719  IP_3075186  female        45.0               upper extremity       0         0\n",
      "2  ISIC_0052212  IP_2842074  female        50.0               lower extremity       0         6\n",
      "3  ISIC_0068279  IP_6890425  female        45.0                     head/neck       0         0\n",
      "4  ISIC_0074268  IP_8723313  female        55.0               upper extremity       0        11\n",
      "****************************************************************************************************\n",
      "\n",
      "\n",
      "\n",
      "Viewing Testing Dataset below\n",
      "\n",
      "     image_name  patient_id     sex  age_approx anatom_site_general_challenge\n",
      "0  ISIC_0052060  IP_3579794    male        70.0                           NaN\n",
      "1  ISIC_0052349  IP_7782715    male        40.0               lower extremity\n",
      "2  ISIC_0058510  IP_7960270  female        55.0                         torso\n",
      "3  ISIC_0073313  IP_6375035  female        50.0                         torso\n",
      "4  ISIC_0073502  IP_0589375  female        45.0               lower extremity\n"
     ]
    }
   ],
   "source": [
    "## Looking at the Data\n",
    "\n",
    "train_df = pd.read_csv(Config.train_csv_path_2020,\n",
    "                       usecols=Config.TRAIN_COLS)\n",
    "print(f\"Number of Data points & Columns in Training dataset are - {train_df.shape}\\n\")\n",
    "\n",
    "test_df = pd.read_csv(Config.test_csv_path_2020,\n",
    "                       usecols=Config.TEST_COLS)\n",
    "print(f\"Number of Data points & Columns in 2020 Testing dataset are - {test_df.shape}\\n\")\n",
    "\n",
    "print(f\"Train Dataset Columns: {Config.TRAIN_COLS}\\n\")\n",
    "print(f\"Test Dataset Columns: {Config.TEST_COLS}\\n\")\n",
    "\n",
    "print(f\"Distribution of Target feature in training dataset are: \\n{train_df['target'].value_counts(normalize=True) * 100}\\n\")\n",
    "print(\"*\"*70 + \"\\n\")\n",
    "\n",
    "print(f\"Distribution of sex feature in training dataset are:\\n{train_df['sex'].value_counts(normalize=True, dropna=False) * 100}\\n\")\n",
    "print(\"*\"*50 + \"\\n\")\n",
    "print(f\"Distribution of sex feature in testing dataset are:\\n{test_df['sex'].value_counts(normalize=True, dropna=False) * 100}\\n\")\n",
    "\n",
    "print(f\"Distribution of anatom_site_general_challenge feature in training dataset are:\\n\\n{train_df['anatom_site_general_challenge'].value_counts(normalize=True, dropna=False) * 100}\\n\")\n",
    "print(\"*\"*70 + \"\\n\")\n",
    "print(f\"Distribution of anatom_site_general_challenge feature in testing dataset are:\\n\\n{test_df['anatom_site_general_challenge'].value_counts(normalize=True, dropna=False) * 100}\")\n",
    "print(\"*\"*70 + \"\\n\")\n",
    "\n",
    "print(f\"------------------- Missing values distribution in training dataset-------------------:\\n{train_df.isnull().sum()}\\n\")\n",
    "print(f\"-------------------Missing values distribution in testing dataset-------------------:\\n{test_df.isnull().sum()}\\n\")\n",
    "\n",
    "print('\\n\\nViewing Training Dataset below\\n')\n",
    "print(train_df.head())\n",
    "print(\"*\"*100 + \"\\n\")\n",
    "print('\\n\\nViewing Testing Dataset below\\n')\n",
    "print(test_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-09T07:21:08.061888Z",
     "iopub.status.busy": "2023-04-09T07:21:08.061238Z",
     "iopub.status.idle": "2023-04-09T07:21:08.189078Z",
     "shell.execute_reply": "2023-04-09T07:21:08.188129Z",
     "shell.execute_reply.started": "2023-04-09T07:21:08.061848Z"
    }
   },
   "outputs": [],
   "source": [
    "## Filling Missing values.\n",
    "train_df['sex'] = train_df['sex'].fillna(\"missing\")\n",
    "test_df['sex'] = test_df['sex'].fillna(\"missing\")\n",
    "train_df['anatom_site_general_challenge'] = train_df['anatom_site_general_challenge'].fillna(\"missing\")\n",
    "test_df['anatom_site_general_challenge'] = test_df['anatom_site_general_challenge'].fillna(\"missing\")\n",
    "train_df['age_approx'] = train_df['age_approx'].fillna(-1)\n",
    "test_df['age_approx'] = test_df['age_approx'].fillna(-1)\n",
    "\n",
    "## Combining Rare categories that appear less than 5%.\n",
    "rare_label = RareLabelCategoryEncoder(tol=0.05, variables=['anatom_site_general_challenge'])\n",
    "rare_label.fit(train_df)\n",
    "train_df = rare_label.transform(train_df)\n",
    "test_df = rare_label.fit_transform(test_df)\n",
    "\n",
    "## Capping outlier values in Age feature.\n",
    "outlier_treat = OutlierTreatment(variable=\"age_approx\", lower_quantile=0.01, upper_quantile=1.0)\n",
    "outlier_treat.fit(train_df)\n",
    "train_df = outlier_treat.transform(train_df)\n",
    "test_df = outlier_treat.transform(test_df)\n",
    "\n",
    "## Replacing characters in AnatomSiteGeneralChallenge feature\n",
    "train_df['anatom_site_general_challenge'] = train_df['anatom_site_general_challenge'].str.replace(\"/\",\"_\")\n",
    "test_df['anatom_site_general_challenge'] = test_df['anatom_site_general_challenge'].str.replace(\"/\",\"_\")\n",
    "train_df['anatom_site_general_challenge'] = train_df['anatom_site_general_challenge'].str.replace(\" \",\"_\")\n",
    "test_df['anatom_site_general_challenge'] = test_df['anatom_site_general_challenge'].str.replace(\" \",\"_\")\n",
    "\n",
    "## Creating OneHotEncoded features on Sex & AnatomSiteGeneralChallene Feature.\n",
    "sex_one_hot = OneHotEncoder(sparse=False, handle_unknown=\"ignore\", drop=\"first\")\n",
    "sex_one_hot.fit(train_df[['sex']])\n",
    "train_sex_features = pd.DataFrame(sex_one_hot.transform(train_df[['sex']]),\n",
    "                                  columns=sex_one_hot.get_feature_names_out().tolist())\n",
    "test_sex_features = pd.DataFrame(sex_one_hot.transform(test_df[['sex']]),\n",
    "                                  columns=sex_one_hot.get_feature_names_out().tolist())\n",
    "\n",
    "anatom_one_hot = OneHotEncoder(sparse=False, handle_unknown=\"ignore\", drop=\"first\")\n",
    "anatom_one_hot.fit(train_df[['anatom_site_general_challenge']])\n",
    "train_anatom_features = pd.DataFrame(anatom_one_hot.transform(train_df[['anatom_site_general_challenge']]), \n",
    "                                     columns=anatom_one_hot.get_feature_names_out().tolist())\n",
    "test_anatom_features = pd.DataFrame(anatom_one_hot.transform(test_df[['anatom_site_general_challenge']]), \n",
    "                                    columns=anatom_one_hot.get_feature_names_out().tolist())\n",
    "\n",
    "## Scaling Age feature\n",
    "age_scaler = StandardScaler()\n",
    "age_scaler.fit(train_df[['age_approx']])\n",
    "train_scaled_age = pd.DataFrame(age_scaler.transform(train_df[['age_approx']]), columns=['scaled_age'])\n",
    "test_scaled_age = pd.DataFrame(age_scaler.transform(test_df[['age_approx']]), columns=['scaled_age'])\n",
    "\n",
    "## Dropping Original features.\n",
    "train_df.drop(['sex', \"anatom_site_general_challenge\", 'age_approx'], axis=1, inplace=True)\n",
    "test_df.drop(['sex', \"anatom_site_general_challenge\", 'age_approx'], axis=1, inplace=True)\n",
    "\n",
    "## Concatenating the Original features.\n",
    "train_df = pd.concat([train_df, train_sex_features, train_anatom_features, train_scaled_age], axis=1)\n",
    "test_df = pd.concat([test_df, test_sex_features, test_anatom_features, test_scaled_age], axis=1)\n",
    "\n",
    "## Defining Tabular-features\n",
    "tabular_features = ['sex_missing', 'anatom_site_general_challenge_head_neck', \n",
    "                   'anatom_site_general_challenge_lower_extremity', 'anatom_site_general_challenge_torso', \n",
    "                   'anatom_site_general_challenge_upper_extremity', 'scaled_age']\n",
    "num_tabular_features = len(tabular_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-09T07:21:08.192647Z",
     "iopub.status.busy": "2023-04-09T07:21:08.192102Z",
     "iopub.status.idle": "2023-04-09T07:21:08.199043Z",
     "shell.execute_reply": "2023-04-09T07:21:08.197921Z",
     "shell.execute_reply.started": "2023-04-09T07:21:08.192600Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_folds(train_df=train_df):\n",
    "    train_df = train_df.loc[train_df['tfrecord'] != -1].reset_index(drop=True)\n",
    "    train_df['fold'] = train_df['tfrecord'] % 5\n",
    "    return train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2023-04-09T07:21:08.201128Z",
     "iopub.status.busy": "2023-04-09T07:21:08.200772Z",
     "iopub.status.idle": "2023-04-09T07:21:08.241604Z",
     "shell.execute_reply": "2023-04-09T07:21:08.240560Z",
     "shell.execute_reply.started": "2023-04-09T07:21:08.201092Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_name</th>\n",
       "      <th>patient_id</th>\n",
       "      <th>target</th>\n",
       "      <th>tfrecord</th>\n",
       "      <th>sex_male</th>\n",
       "      <th>sex_missing</th>\n",
       "      <th>anatom_site_general_challenge_head_neck</th>\n",
       "      <th>anatom_site_general_challenge_lower_extremity</th>\n",
       "      <th>anatom_site_general_challenge_torso</th>\n",
       "      <th>anatom_site_general_challenge_upper_extremity</th>\n",
       "      <th>scaled_age</th>\n",
       "      <th>image_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ISIC_2637011</td>\n",
       "      <td>IP_7279968</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.267090</td>\n",
       "      <td>/kaggle/input/jpeg-melanoma-512x512/train/ISIC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ISIC_0015719</td>\n",
       "      <td>IP_3075186</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.267090</td>\n",
       "      <td>/kaggle/input/jpeg-melanoma-512x512/train/ISIC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ISIC_0052212</td>\n",
       "      <td>IP_2842074</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.080954</td>\n",
       "      <td>/kaggle/input/jpeg-melanoma-512x512/train/ISIC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ISIC_0068279</td>\n",
       "      <td>IP_6890425</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.267090</td>\n",
       "      <td>/kaggle/input/jpeg-melanoma-512x512/train/ISIC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ISIC_0074268</td>\n",
       "      <td>IP_8723313</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.428998</td>\n",
       "      <td>/kaggle/input/jpeg-melanoma-512x512/train/ISIC...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     image_name  patient_id  target  tfrecord  sex_male  sex_missing  anatom_site_general_challenge_head_neck  anatom_site_general_challenge_lower_extremity  anatom_site_general_challenge_torso  anatom_site_general_challenge_upper_extremity  scaled_age                                         image_path\n",
       "0  ISIC_2637011  IP_7279968       0         0       1.0          0.0                                      1.0                                            0.0                                  0.0                                            0.0   -0.267090  /kaggle/input/jpeg-melanoma-512x512/train/ISIC...\n",
       "1  ISIC_0015719  IP_3075186       0         0       0.0          0.0                                      0.0                                            0.0                                  0.0                                            1.0   -0.267090  /kaggle/input/jpeg-melanoma-512x512/train/ISIC...\n",
       "2  ISIC_0052212  IP_2842074       0         6       0.0          0.0                                      0.0                                            1.0                                  0.0                                            0.0    0.080954  /kaggle/input/jpeg-melanoma-512x512/train/ISIC...\n",
       "3  ISIC_0068279  IP_6890425       0         0       0.0          0.0                                      1.0                                            0.0                                  0.0                                            0.0   -0.267090  /kaggle/input/jpeg-melanoma-512x512/train/ISIC...\n",
       "4  ISIC_0074268  IP_8723313       0        11       0.0          0.0                                      0.0                                            0.0                                  0.0                                            1.0    0.428998  /kaggle/input/jpeg-melanoma-512x512/train/ISIC..."
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Creating Image_Path for each images in 2019 & 2020 training datasets\n",
    "train_df['image_path'] = os.path.join(Config.train_folder_2020) + train_df['image_name'] + \".jpg\"\n",
    "test_df['image_path'] = os.path.join(Config.test_folder_2020) + test_df['image_name'] + \".jpg\"\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-09T07:21:13.716550Z",
     "iopub.status.busy": "2023-04-09T07:21:13.715885Z",
     "iopub.status.idle": "2023-04-09T07:21:13.737542Z",
     "shell.execute_reply": "2023-04-09T07:21:13.736366Z",
     "shell.execute_reply.started": "2023-04-09T07:21:13.716503Z"
    }
   },
   "outputs": [],
   "source": [
    "def run_model(fold, train_df):\n",
    "    train_df = create_folds(train_df=train_df)\n",
    "    train_data = train_df.loc[train_df['fold'] != fold].reset_index(drop=True)\n",
    "    valid_data = train_df.loc[train_df['fold'] == fold].reset_index(drop=True)\n",
    "    validation_targets = valid_data['target']\n",
    "    train_dataset = DatasetRetriever(df=train_data, tabular_features=tabular_features, use_tabular_features=True, \n",
    "                     augmentations=training_augmentations, is_test=False)\n",
    "    valid_dataset = DatasetRetriever(df=valid_data, tabular_features=tabular_features, use_tabular_features=True, \n",
    "                     augmentations=validation_augmentations, is_test=False)\n",
    "    training_dataloader = DataLoader(dataset=train_dataset, batch_size=Config.TRAIN_BATCH_SIZE, \n",
    "                                     shuffle=True, num_workers=os.cpu_count())\n",
    "    validation_dataloader = DataLoader(dataset=valid_dataset, batch_size=Config.VAL_BATCH_SIZE,\n",
    "                                       shuffle=False, num_workers=os.cpu_count())\n",
    "    seed_everything(Config.RANDOM_STATE)\n",
    "    if torch.cuda.device_count() in (0,1):\n",
    "        model = Model(num_tabular_features=num_tabular_features).to(Config.DEVICE)\n",
    "    elif torch.cuda.device_count() > 1:\n",
    "        model = Model(num_tabular_features=num_tabular_features).to(Config.DEVICE)\n",
    "        model = nn.DataParallel(model)\n",
    "    loss = nn.BCEWithLogitsLoss()\n",
    "    optimizer = torch.optim.AdamW(params=model.parameters(), lr=Config.LEARNING_RATE, weight_decay=Config.WEIGHT_DECAY)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer, \n",
    "                                    mode='max', factor=0.2, patience=2, \n",
    "                                    threshold=1e-3,verbose=True)\n",
    "    scaler = amp.GradScaler()\n",
    "    start_time = timer()\n",
    "    model_save_path = f\"Models/efficientnet_b5_checkpoint_fold_{fold}.pt\"\n",
    "    model_results = train(model=model, train_dataloader=training_dataloader, \n",
    "                            valid_dataloader=validation_dataloader,\n",
    "                            loss_fn = loss, optimizer=optimizer, scheduler=scheduler,\n",
    "                            device=Config.DEVICE, scaler=scaler,\n",
    "                            epochs=Config.EPOCHS, es_patience=2, \n",
    "    model_save_path=model_save_path, validation_targets=validation_targets)\n",
    "    end_time = timer()\n",
    "    print(f\"Total training time: {end_time-start_time:.3f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-09T11:45:14.756861Z",
     "iopub.status.busy": "2023-04-09T11:45:14.755834Z",
     "iopub.status.idle": "2023-04-09T11:45:14.770912Z",
     "shell.execute_reply": "2023-04-09T11:45:14.769573Z",
     "shell.execute_reply.started": "2023-04-09T11:45:14.756822Z"
    }
   },
   "outputs": [],
   "source": [
    "def validate(fold, train_df, use_tabular_features=True):\n",
    "    full_train_df = create_folds(train_df)\n",
    "    valid_data = full_train_df.loc[full_train_df['fold'] == fold].reset_index(drop=True)\n",
    "    validation_targets = valid_data['target']\n",
    "    valid_dataset = DatasetRetriever(df=valid_data, tabular_features=tabular_features, use_tabular_features=True, \n",
    "                     augmentations=validation_augmentations, is_test=False)\n",
    "    validation_dataloader = DataLoader(dataset=valid_dataset, batch_size=Config.VAL_BATCH_SIZE,\n",
    "                                       shuffle=False, num_workers=os.cpu_count())\n",
    "    valid_predictions = []\n",
    "    if torch.cuda.device_count() in (0,1):\n",
    "        model = Model(num_tabular_features=num_tabular_features).to(Config.DEVICE)\n",
    "    elif torch.cuda.device_count() > 1:\n",
    "        model = Model(num_tabular_features=num_tabular_features).to(Config.DEVICE)\n",
    "        model = nn.DataParallel(model)\n",
    "    model.load_state_dict(torch.load(f\"/kaggle/working/Models/efficientnet_b5_checkpoint_fold_{fold}.pt\"))\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        for batch, data in enumerate(validation_dataloader):\n",
    "            if use_tabular_features:\n",
    "                data[\"image\"], data[\"tabular_features\"], data['targets'] = data[\"image\"].to(Config.DEVICE, dtype=torch.float), \\\n",
    "                    data[\"tabular_features\"].to(Config.DEVICE, dtype=torch.float), data['targets'].to(Config.DEVICE, dtype=torch.float)\n",
    "                y_logits = model(data['image'], data['tabular_features']).squeeze(dim=0)\n",
    "            else:   \n",
    "                data[\"image\"], data['targets'] = data[\"image\"].to(Config.DEVICE, dtype=torch.float),\\\n",
    "                    data['targets'].to(Config.DEVICE, dtype=torch.float)    \n",
    "                y_logits = model(data['image']).squeeze(dim=0)\n",
    "            valid_probs = torch.sigmoid(y_logits).detach().cpu().numpy()\n",
    "            valid_predictions.extend(valid_probs)\n",
    "    valid_auc = roc_auc_score(y_true=validation_targets, y_score=valid_predictions)\n",
    "    print(valid_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-09T11:45:15.474408Z",
     "iopub.status.busy": "2023-04-09T11:45:15.474011Z",
     "iopub.status.idle": "2023-04-09T11:45:15.487606Z",
     "shell.execute_reply": "2023-04-09T11:45:15.484434Z",
     "shell.execute_reply.started": "2023-04-09T11:45:15.474373Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict_on_test(fold, use_tabular_features=True):\n",
    "    test_dataset = DatasetRetriever(df=test_df, tabular_features=tabular_features, use_tabular_features=True, \n",
    "                     augmentations=testing_augmentations, is_test=True)\n",
    "    test_dataloader = DataLoader(dataset=test_dataset, batch_size=Config.VAL_BATCH_SIZE,\n",
    "                                       shuffle=False, num_workers=os.cpu_count())\n",
    "    test_predictions = []\n",
    "    if torch.cuda.device_count() in (0,1):\n",
    "        model = Model(num_tabular_features=num_tabular_features).to(Config.DEVICE)\n",
    "    elif torch.cuda.device_count() > 1:\n",
    "        model = Model(num_tabular_features=num_tabular_features).to(Config.DEVICE)\n",
    "        model = nn.DataParallel(model)\n",
    "    model.load_state_dict(torch.load(f\"/kaggle/working/Models/efficientnet_b5_checkpoint_fold_{fold}.pt\"))   \n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        for batch, data in enumerate(test_dataloader):\n",
    "            if use_tabular_features:\n",
    "                data[\"image\"], data[\"tabular_features\"] = data[\"image\"].to(Config.DEVICE, dtype=torch.float), \\\n",
    "                        data[\"tabular_features\"].to(Config.DEVICE, dtype=torch.float)\n",
    "                y_logits = model(data['image'], data['tabular_features'])\n",
    "            else:\n",
    "                data[\"image\"] = data[\"image\"].to(Config.DEVICE, dtype=torch.float)    \n",
    "                y_logits = model(data['image']).squeeze(dim=0)\n",
    "            test_probs = torch.sigmoid(y_logits).detach().cpu().numpy()\n",
    "            test_predictions.extend(test_probs)\n",
    "    submission_df = pd.read_csv(Config.submission_csv_path)\n",
    "    test_predictions = [test_predictions[img].item() for img in range(len(test_predictions))]\n",
    "    submission_df['target'] = test_predictions\n",
    "    submission_df.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2023-04-09T07:21:15.604540Z",
     "iopub.status.busy": "2023-04-09T07:21:15.603930Z",
     "iopub.status.idle": "2023-04-09T11:43:24.281672Z",
     "shell.execute_reply": "2023-04-09T11:43:24.279065Z",
     "shell.execute_reply.started": "2023-04-09T07:21:15.604501Z"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/lukemelas/EfficientNet-PyTorch/releases/download/1.0/efficientnet-b5-b6417697.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet-b5-b6417697.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43c192b237bf4ef386fecc638de9e709",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/117M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using tabular features\n",
      "Using tabular features\n",
      "Validation loss decreased (inf --> 0.073976).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 1/20 [52:13<16:32:12, 3133.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1 | train_loss : 0.0958 | valid_loss : 0.0740 | valid_auc : 0.8652 \n",
      "Using tabular features\n",
      "Using tabular features\n",
      "Validation loss decreased (0.073976 --> 0.064940).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 2/20 [1:44:53<15:44:40, 3148.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 2 | train_loss : 0.0727 | valid_loss : 0.0649 | valid_auc : 0.8979 \n",
      "Using tabular features\n",
      "Using tabular features\n",
      "Validation loss decreased (0.064940 --> 0.060836).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 3/20 [2:37:22<14:52:17, 3149.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 3 | train_loss : 0.0690 | valid_loss : 0.0608 | valid_auc : 0.9183 \n",
      "Using tabular features\n",
      "Using tabular features\n",
      "EarlyStopping counter: 1 out of 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 4/20 [3:29:46<13:59:15, 3147.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 4 | train_loss : 0.0670 | valid_loss : 0.0640 | valid_auc : 0.9032 \n",
      "Using tabular features\n",
      "Using tabular features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 4/20 [4:22:02<17:28:09, 3930.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 2 out of 2\n",
      "Early Stopping\n",
      "Total training time: 15722.408 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "run_model(fold=0, train_df=train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-09T11:45:21.494701Z",
     "iopub.status.busy": "2023-04-09T11:45:21.494338Z",
     "iopub.status.idle": "2023-04-09T11:47:34.140370Z",
     "shell.execute_reply": "2023-04-09T11:47:34.138027Z",
     "shell.execute_reply.started": "2023-04-09T11:45:21.494669Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b5\n",
      "0.9182890775448209\n"
     ]
    }
   ],
   "source": [
    "validate(fold=0, train_df=train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-09T11:47:35.824741Z",
     "iopub.status.busy": "2023-04-09T11:47:35.824008Z",
     "iopub.status.idle": "2023-04-09T11:51:21.181734Z",
     "shell.execute_reply": "2023-04-09T11:51:21.180565Z",
     "shell.execute_reply.started": "2023-04-09T11:47:35.824693Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b5\n"
     ]
    }
   ],
   "source": [
    "predict_on_test(fold=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model gave a score of 0.8828 on private leadeboard & 0.8942 on public leaderboard."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
